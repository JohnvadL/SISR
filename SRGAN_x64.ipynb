{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN x64.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OhG73ITGWndz"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxmteZYymEGT",
        "outputId": "ffd57d32-7a32-420d-c7bc-2273c79e5b40"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEzBqrqD1a57"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import pickle\n",
        "import numpy as np \n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.optim\n",
        "from scipy.io.idl import AttrDict\n",
        "\n",
        "from PIL import Image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhG73ITGWndz"
      },
      "source": [
        "# IMAGE UNZIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8tOndz6lL70"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "from os import walk\n",
        "\n",
        "extract_x64 = False \n",
        "\n",
        "if extract_x64: \n",
        "  _, _, filenames = next(walk(\".\"))\n",
        "\n",
        "  for filename in filenames: \n",
        "    print(filename)\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "      zip_ref.extractall(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aThIZDMBXMSs"
      },
      "source": [
        "# ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpPBcP5m_awL"
      },
      "source": [
        "class generator (torch.nn.Module):\n",
        "\n",
        "  def __init__( self):\n",
        "    super(generator, self).__init__()\n",
        "    self.Conv1 = torch.nn.Conv2d(3, 64, 9, padding=4)\n",
        "\n",
        "    self.residual_block = torch.nn.Sequential()\n",
        "    for i in range(16):\n",
        "      self.residual_block = torch.nn.Sequential(*list(self.residual_block) + [ResidualBlock()])\n",
        "\n",
        "    self.Conv2 = torch.nn.Conv2d(64, 64, 3, padding=1 )\n",
        "    self.Conv3 = torch.nn.Conv2d(64, 256, 3, padding=1 )\n",
        "    self.Conv4 = torch.nn.Conv2d(64, 256, 3, padding=1 )\n",
        "    self.ConvFinal = torch.nn.Conv2d(64, 3, 9, padding=4)\n",
        "    self.PixelShuffle = torch.nn.PixelShuffle(upscale_factor=2)\n",
        "    self.PreLU = torch.nn.PReLU();\n",
        "    self.BatchNorm = torch.nn.BatchNorm2d(num_features=64)\n",
        "\n",
        "\n",
        "  def forward(self, x ):\n",
        "\n",
        "    x = self.Conv1(x)\n",
        "    x = self.PreLU(x)\n",
        "    \n",
        "    x1 = x\n",
        "    x2 = x\n",
        "\n",
        "    self.residual_block(x1)\n",
        "\n",
        "    x = self.Conv2(x)\n",
        "    x = self.BatchNorm(x)\n",
        "    x = x + x2 \n",
        "\n",
        "    x = self.Conv3(x)\n",
        "    x = self.PixelShuffle(x)\n",
        "    x = self.PreLU(x)\n",
        "    \n",
        "    x = self.Conv4(x)\n",
        "    x = self.PixelShuffle(x)\n",
        "    x = self.PreLU(x)\n",
        "\n",
        "    x = self.ConvFinal(x)\n",
        "    return x \n",
        "\n",
        "\n",
        "\n",
        "class ResidualBlock (torch.nn.Module):\n",
        "  def __init__( self):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.block = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(64, 64, 3, padding=1),\n",
        "        torch.nn.BatchNorm2d(64),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.Conv2d(64, 64, 3, padding=1),\n",
        "        torch.nn.BatchNorm2d(64),\n",
        "    )\n",
        "\n",
        "  def forward(self, x ):\n",
        "    return self.block(x) + x \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYFT-UojGC8Q"
      },
      "source": [
        "class discriminator (torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(discriminator, self).__init__()\n",
        "    self.Conv1 = torch.nn.Conv2d(3, 64, 3)\n",
        "    self.Conv2 = torch.nn.Conv2d(64, 64, 3, stride=2)\n",
        "    self.BatchNorm2 = torch.nn.BatchNorm2d(num_features=64)\n",
        "    self.Conv3 = torch.nn.Conv2d(64, 128, 3)\n",
        "    self.BatchNorm3 = torch.nn.BatchNorm2d(num_features=128)\n",
        "    self.Conv4 = torch.nn.Conv2d(128, 128, 3, 2)\n",
        "    self.BatchNorm4 = torch.nn.BatchNorm2d(num_features=128)\n",
        "    self.Conv5 = torch.nn.Conv2d(128, 256, kernel_size=3)\n",
        "    self.BatchNorm5 = torch.nn.BatchNorm2d(num_features=256)\n",
        "    self.Conv6 = torch.nn.Conv2d(256, 256, kernel_size=3,stride=2)\n",
        "    self.BatchNorm6 = torch.nn.BatchNorm2d(num_features=256)\n",
        "    self.Conv7 = torch.nn.Conv2d(256, 512, kernel_size=3)\n",
        "    self.BatchNorm7 = torch.nn.BatchNorm2d(num_features=512)\n",
        "    self.Conv8 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=2)\n",
        "    self.BatchNorm8 = torch.nn.BatchNorm2d(num_features=512)\n",
        "\n",
        "    self.hidden = torch.nn.Linear(in_features=512, out_features=1024)\n",
        "    self.final_classifier = torch.nn.Linear(in_features=1024, out_features=1)\n",
        "    self.leakyReLU = torch.nn.LeakyReLU(negative_slope=0.2)\n",
        "    self.dropout = torch.nn.Dropout(p=0.3)\n",
        "\n",
        "\n",
        "  def forward(self, x ):\n",
        "    x = self.leakyReLU(self.Conv1(x))\n",
        "    x = self.leakyReLU(self.BatchNorm2(self.Conv2(x)))\n",
        "    x = self.leakyReLU(self.BatchNorm3(self.Conv3(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = self.leakyReLU(self.BatchNorm4(self.Conv4(x)))\n",
        "    x = self.leakyReLU(self.BatchNorm5(self.Conv5(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = self.leakyReLU(self.BatchNorm6(self.Conv6(x)))\n",
        "    x = self.leakyReLU(self.BatchNorm7(self.Conv7(x)))\n",
        "    x = self.leakyReLU(self.BatchNorm8(self.Conv8(x)))\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = self.dropout(x)\n",
        "    x = self.leakyReLU(self.hidden(x))\n",
        "    x = self.final_classifier(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlzccsRnRqgL"
      },
      "source": [
        "class vgg_loss ():\n",
        "\n",
        "  def __init__(self, device):\n",
        "    super(vgg_loss, self).__init__()\n",
        "    vgg = torchvision.models.vgg19(pretrained=True)\n",
        "    vgg = vgg.features\n",
        "\n",
        "    # Freeze all vgg layers \n",
        "    for param in vgg.parameters():\n",
        "      param.requires_grad = False \n",
        "    \n",
        "    self.vgg = vgg[:27].to(device) \n",
        "    self.MSE = torch.nn.MSELoss()\n",
        "\n",
        "  def __call__(self, input ):\n",
        "    return self.vgg(input)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwxNZygOXTj6"
      },
      "source": [
        "# DATA LOADERS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySLjI6M1zKQK"
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo)\n",
        "    return dict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOL2ESHyynZ-"
      },
      "source": [
        "def get_images_x64(filename):\n",
        "  d = unpickle(filename)\n",
        "  x = d['data']\n",
        "  x = np.dstack((x[:, :4096], x[:, 4096:8192], x[:, 8192:]))\n",
        "  x = x.reshape((x.shape[0], 64, 64, 3))\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jyRA29KEe0p"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/CSC413/x64/'\n",
        "\n",
        "class ImageNetSR(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, train=False,  batch=\"\"):\n",
        "        self.train = train\n",
        "        self.transform = torchvision.transforms.ToTensor() \n",
        "        self.downsample = torchvision.transforms.Resize((16,16), interpolation=torchvision.transforms.InterpolationMode.BICUBIC)\n",
        "\n",
        "        if train: \n",
        "          self.dir = data_dir + batch\n",
        "        else: \n",
        "          self.dir = data_dir + \"val_data\" \n",
        "\n",
        "        self.X = get_images_x64( self.dir)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        image = self.transform(x)\n",
        "        downsampled = self.downsample(image)\n",
        "        sample = {'HR': image, 'LR': downsampled}\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3SCmhEjXmlN"
      },
      "source": [
        "# TRAINING CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7X1MNMbYpCk"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfjpXNYyAgcY"
      },
      "source": [
        "def gan_checkpoint(path, G, D):\n",
        "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(path, 'G.pkl')\n",
        "    D_path = os.path.join(path, 'D.pkl')\n",
        "    torch.save(G.state_dict(), G_path)\n",
        "    torch.save(D.state_dict(), D_path)\n",
        "\n",
        "def load_checkpoint(opts):\n",
        "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.load, 'G.pkl')\n",
        "    D_path = os.path.join(opts.load, 'D.pkl')\n",
        "\n",
        "    G = generator()\n",
        "    D = discriminator()\n",
        "\n",
        "    G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
        "    D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "    G.to(opts.device)\n",
        "\n",
        "    return G, D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ZD24cR2Ji8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOGoKyuSACfx"
      },
      "source": [
        "\"\"\"\n",
        "CREDITS: CSC413 PA4 DCGAN\n",
        "\"\"\"\n",
        "\n",
        "def train(opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "    # Create generators and discriminators\n",
        "    G = generator().to(opts.device)\n",
        "    D = discriminator().to(opts.device)\n",
        "\n",
        "    g_params = G.parameters()  # Get generator parameters\n",
        "    d_params = D.parameters()  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = torch.optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = torch.optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "\n",
        "    print(\"loading training ... \")\n",
        "    batch = 1 \n",
        "    train_set = ImageNetSR(train=True, batch=\"train_data_batch_\" + str(batch))\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, opts.batch_size,\n",
        "                              shuffle=True, num_workers=opts.num_workers)\n",
        "\n",
        "    print(\"loading validation ... \")\n",
        "    val_set = ImageNetSR(train=False)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, opts.batch_size,\n",
        "                              shuffle=True, num_workers=opts.num_workers)\n",
        "\n",
        "\n",
        "    train_iter = iter(train_loader)\n",
        "    test_iter = iter(val_loader)\n",
        "\n",
        "    iter_per_epoch = len(train_iter)\n",
        "    total_train_iters = opts.train_iters\n",
        " \n",
        "    # adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
        "    gp_weight = 1\n",
        "\n",
        "    losses ={}\n",
        "    losses['iteration'] = []\n",
        "    losses['D_loss'] = []\n",
        "    losses['G_loss'] = []\n",
        "\n",
        "\n",
        "    VGG = vgg_loss(opts.device)\n",
        "    MSE = torch.nn.MSELoss()\n",
        "    BCE = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "    data_transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(256),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "    image_name = \"/content/drive/MyDrive/CSC413/LR_Test.jpg\"\n",
        "    og_image = Image.open(image_name)\n",
        "    image = data_transforms(og_image).float()\n",
        "    image = torch.tensor(image, requires_grad=True)\n",
        "\n",
        "    image = image.unsqueeze(0).to(opts.device)\n",
        "\n",
        "\n",
        "    try:\n",
        "        for iteration in range(1, opts.train_iters + 1):\n",
        "\n",
        "            G.train()\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # Reset data_iter for each epoch\n",
        "            if iteration % iter_per_epoch == 0:\n",
        "              \n",
        "              if batch == 10: batch = 0\n",
        "\n",
        "              batch += 1\n",
        "              print(\"loading train_data_batch_\" + str(batch))\n",
        "              train_set = ImageNetSR(train=True, batch=\"train_data_batch_\" + str(batch))\n",
        "              train_loader = torch.utils.data.DataLoader(train_set, opts.batch_size,\n",
        "                                        shuffle=True, num_workers=opts.num_workers)\n",
        "              train_iter = iter(train_loader)\n",
        "              \n",
        "\n",
        "            next = train_iter.next()\n",
        "            HR =  next['HR']\n",
        "            LR = next['LR']\n",
        "            HR = HR.to(opts.device)\n",
        "            LR = LR.to(opts.device)\n",
        "\n",
        "            # ------------------------ DISCRIMINATOR LOSS ---------------------\n",
        "\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "  \n",
        "            HRE = G(LR)\n",
        "            logits_HRE = D(HRE)\n",
        "            logits_HR = D(HR)\n",
        "            \n",
        "            D_1 = torch.mean(logits_HR)\n",
        "            D_2 = torch.mean(logits_HRE)                \n",
        "            D_loss = 1 - D_1 + D_2 \n",
        "\n",
        "            if D_loss > 0.1: \n",
        "              D_loss.backward(retain_graph=True)\n",
        "              d_optimizer.step()\n",
        "\n",
        "\n",
        "            # ------------------------ GENERATOR LOSS -----------------------\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            HRE = G(LR)\n",
        "            logits_HRE = D(HRE)\n",
        "\n",
        "            vgg_HR = VGG(HR)\n",
        "            vgg_HRE = VGG(HRE)\n",
        "\n",
        "            G_1 = torch.mean(1 - logits_HRE)\n",
        "            G_VGG = MSE(vgg_HRE, vgg_HR)\n",
        "            G_MSE = MSE(HRE, HR)\n",
        "            G_loss = G_MSE + 0.006 * G_VGG + 0.001 * G_1 \n",
        "            G_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "            # ------------------------  CHECKPOINTS & SAMPLING -------------------\n",
        "\n",
        "            if iteration % opts.log_step == 0:\n",
        "                losses['iteration'].append(iteration)\n",
        "                losses['D_loss'].append(D_loss.item())\n",
        "                losses['G_loss'].append(G_loss.item())\n",
        "                print('Iteration [{:4d}/{:4d}] | D_loss: {:6.4f} |  G_loss: {:6.4f}'.format(\n",
        "                    iteration, total_train_iters, D_loss.item(), G_loss.item()))\n",
        "\n",
        "            # Save the model parameters\n",
        "            if iteration % opts.checkpoint_every == 0:\n",
        "                checkpoint = opts.checkpoint\n",
        "\n",
        "                dir = checkpoint + str(iteration)\n",
        "                if os.path.exists(dir):\n",
        "                  shutil.rmtree(dir)\n",
        "                os.mkdir(dir)\n",
        "                gan_checkpoint(dir, G, D)\n",
        "\n",
        "                G.eval()\n",
        "\n",
        "                image_est = (G(image).clone().detach().cpu().numpy())\n",
        "                image_est = np.squeeze(image_est, axis=0)\n",
        "                image_est = np.transpose(image_est, (1,2,0) )\n",
        "\n",
        "                f, axarr = plt.subplots(1,2)\n",
        "                axarr[0].imshow(og_image)\n",
        "                axarr[1].imshow(image_est)\n",
        "                f.savefig(dir + '/sample.png', bbox_inches='tight')\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G, D\n",
        "\n",
        "    return G, D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qPEimgKYsD_"
      },
      "source": [
        "## Hyperparameters and running loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAZaLJsY0J5"
      },
      "source": [
        "opts = AttrDict()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "args_dict = {\n",
        "    'checkpoint': \"/content/drive/MyDrive/CSC413/checkpoints/\",\n",
        "    'lr': 0.0001,\n",
        "    'beta1':0.9,\n",
        "    'beta2':0.999,\n",
        "    'batch_size': 32,\n",
        "    'device': device, \n",
        "    'epochs': 12,\n",
        "    'num_workers': 4,\n",
        "    'resume': False,\n",
        "    'log_step': 100, \n",
        "    'checkpoint_every':1000,\n",
        "    'train_iters':200000,\n",
        "    'batch' : 'train_data_batch_1'\n",
        "}\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "opts.update(args_dict)\n",
        "train(opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfcW5Zno4wXu"
      },
      "source": [
        "# TEST IMAGE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw_CtcuN1RCh"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "from tifffile import imsave\n",
        "import cv2 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def torch_to_saveable_image(image_est):\n",
        "  image_est = image_est.cpu().clone().detach().numpy()\n",
        "  image_est = np.transpose(image_est, (1,2,0))\n",
        "  image_est = cv2.normalize(image_est, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "  image_est = cv2.cvtColor(image_est, cv2.COLOR_RGB2BGR)\n",
        "  return image_est\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "load_opts = AttrDict()\n",
        "args_dict = {\n",
        "    'load': '/content/drive/MyDrive/CSC413/checkpoints/17000',\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "load_opts.update(args_dict)\n",
        "\n",
        "\n",
        "G, _ = load_checkpoint(load_opts)\n",
        "\n",
        "data_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((64,64))\n",
        "])\n",
        "\n",
        "\n",
        "filename = \"obama\"\n",
        "filetype = \".jpg\"\n",
        "directory = \"/content/drive/MyDrive/CSC413/checkpoints/172000/\"\n",
        "\n",
        "\n",
        "image_name = directory + filename  + filetype\n",
        "og_image = Image.open(image_name)\n",
        "image = data_transforms(og_image).float()\n",
        "\n",
        "\n",
        "image = torch.tensor(image, requires_grad=True)\n",
        "image = image.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "\n",
        "image_est = G(image).squeeze(0)\n",
        "image_est = torch_to_saveable_image(image_est)\n",
        "\n",
        "\n",
        "\n",
        "bicubic = torchvision.transforms.Resize((256,256))\n",
        "image =  image.squeeze(0)\n",
        "bicubic = bicubic(image)\n",
        "bicubic = torch_to_saveable_image(bicubic)\n",
        "\n",
        "plt.imshow(image_est)\n",
        "cv2.imwrite(directory + filename + \"_SRGAN.png\", image_est)\n",
        "cv2.imwrite(directory + filename + \"_bicubic.png\", bicubic)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}